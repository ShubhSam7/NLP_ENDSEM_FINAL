{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Market Sentiment Analysis - Future Work Demo\n",
    "\n",
    "This notebook demonstrates the advanced features added to the FinLlama ensemble system:\n",
    "1. Time Series Analysis & Forecasting\n",
    "2. Portfolio Optimization\n",
    "3. Real-time Alert System\n",
    "4. Interactive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from future_work_implementation import (\n",
    "    TimeSeriesAnalyzer,\n",
    "    PortfolioOptimizer,\n",
    "    SentimentAlertSystem,\n",
    "    EnhancedVisualizer\n",
    ")\n",
    "from integration import SentimentPipeline\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Load sentiment data from your ensemble system output or generate sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load your actual ensemble output\n",
    "# sentiment_df = pd.read_csv('path/to/your/ensemble_output.csv')\n",
    "# sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "\n",
    "# Option 2: Generate sample data for demonstration\n",
    "def generate_sample_data(companies=['AAPL', 'GOOGL', 'MSFT', 'TSLA'], days=90):\n",
    "    np.random.seed(42)\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    all_data = []\n",
    "    for company in companies:\n",
    "        # Random walk sentiment\n",
    "        sentiment = np.random.randn(len(dates)).cumsum() * 0.05\n",
    "        sentiment = np.clip(sentiment, -1, 1)\n",
    "        \n",
    "        for i, date in enumerate(dates):\n",
    "            all_data.append({\n",
    "                'date': date,\n",
    "                'company': company,\n",
    "                'sentiment_score': sentiment[i],\n",
    "                'confidence': np.random.uniform(0.6, 0.95),\n",
    "                'volume': np.random.randint(100, 1000)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# Generate sample data\n",
    "full_df = generate_sample_data()\n",
    "\n",
    "print(f\"Generated {len(full_df)} sentiment records\")\n",
    "print(f\"Companies: {full_df['company'].unique().tolist()}\")\n",
    "print(f\"Date range: {full_df['date'].min()} to {full_df['date'].max()}\")\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data by company\n",
    "companies = full_df['company'].unique().tolist()\n",
    "sentiment_data = {}\n",
    "\n",
    "for company in companies:\n",
    "    company_df = full_df[full_df['company'] == company].copy()\n",
    "    company_df = company_df.sort_values('date').reset_index(drop=True)\n",
    "    sentiment_data[company] = company_df\n",
    "\n",
    "print(f\"Prepared data for {len(sentiment_data)} companies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Analysis & Forecasting\n",
    "\n",
    "Train LSTM models to forecast future sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = TimeSeriesAnalyzer(sequence_length=10)\n",
    "\n",
    "# Select a company for detailed analysis\n",
    "company = 'AAPL'\n",
    "df = sentiment_data[company]\n",
    "\n",
    "print(f\"Analyzing {company} with {len(df)} days of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model\n",
    "print(\"Training LSTM model...\")\n",
    "history = analyzer.train_lstm_model(\n",
    "    df,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Training Progress (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {history['train_loss'][-1]:.6f}\")\n",
    "print(f\"Final validation loss: {history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "forecast_days = 14\n",
    "predictions = analyzer.predict_future_sentiment(df, days_ahead=forecast_days)\n",
    "\n",
    "print(f\"Generated {forecast_days}-day forecast:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction intervals\n",
    "intervals = analyzer.calculate_prediction_intervals(df, days_ahead=7, confidence=0.95)\n",
    "\n",
    "print(\"Prediction intervals (95% confidence):\")\n",
    "print(intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Historical sentiment\n",
    "plt.plot(df['date'], df['sentiment_score'], 'b-', label='Historical', linewidth=2)\n",
    "\n",
    "# Predictions\n",
    "plt.plot(predictions['date'], predictions['predicted_sentiment'], \n",
    "         'r--', label='Forecast', linewidth=2, marker='o')\n",
    "\n",
    "# Confidence intervals\n",
    "if 'lower_bound' in intervals.columns:\n",
    "    plt.fill_between(\n",
    "        intervals['date'],\n",
    "        intervals['lower_bound'],\n",
    "        intervals['upper_bound'],\n",
    "        alpha=0.3,\n",
    "        color='red',\n",
    "        label='95% Confidence Interval'\n",
    "    )\n",
    "\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.title(f'{company} Sentiment Forecast')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Portfolio Optimization\n",
    "\n",
    "Use sentiment signals to optimize portfolio allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = PortfolioOptimizer(risk_free_rate=0.02)\n",
    "\n",
    "# Get latest sentiment scores\n",
    "sentiment_scores = {\n",
    "    company: df['sentiment_score'].iloc[-1]\n",
    "    for company, df in sentiment_data.items()\n",
    "}\n",
    "\n",
    "print(\"Current Sentiment Scores:\")\n",
    "for company, score in sentiment_scores.items():\n",
    "    sentiment_label = \"Positive\" if score > 0 else \"Negative\"\n",
    "    print(f\"  {company}: {score:+.3f} ({sentiment_label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample returns data\n",
    "dates = pd.date_range(\n",
    "    start=min([df['date'].min() for df in sentiment_data.values()]),\n",
    "    end=max([df['date'].max() for df in sentiment_data.values()]),\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "returns_data = pd.DataFrame({\n",
    "    company: np.random.randn(len(dates)) * 0.02 + 0.001\n",
    "    for company in companies\n",
    "})\n",
    "\n",
    "print(f\"Generated returns data: {returns_data.shape}\")\n",
    "print(\"\\nMean daily returns:\")\n",
    "print(returns_data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment-weighted expected returns\n",
    "expected_returns = optimizer.calculate_sentiment_weighted_returns(\n",
    "    sentiment_scores, returns_data\n",
    ")\n",
    "\n",
    "print(\"\\nSentiment-Adjusted Expected Returns:\")\n",
    "for company, ret in zip(companies, expected_returns):\n",
    "    sentiment = sentiment_scores[company]\n",
    "    print(f\"  {company}: {ret:.4f} (Sentiment: {sentiment:+.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance matrix\n",
    "cov_matrix = returns_data.cov().values\n",
    "\n",
    "# Optimize portfolio for maximum Sharpe ratio\n",
    "optimal_sharpe = optimizer.optimize_portfolio_sharpe(expected_returns, cov_matrix)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMAL PORTFOLIO (Maximum Sharpe Ratio)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAllocation:\")\n",
    "for i, company in enumerate(companies):\n",
    "    weight = optimal_sharpe['weights'][i]\n",
    "    if weight > 0.01:\n",
    "        print(f\"  {company:6s}: {weight:6.2%}\")\n",
    "\n",
    "print(f\"\\nExpected Return: {optimal_sharpe['expected_return']:.4f}\")\n",
    "print(f\"Volatility:      {optimal_sharpe['volatility']:.4f}\")\n",
    "print(f\"Sharpe Ratio:    {optimal_sharpe['sharpe_ratio']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate efficient frontier\n",
    "print(\"Generating efficient frontier...\")\n",
    "frontier_df = optimizer.generate_efficient_frontier(\n",
    "    expected_returns, cov_matrix, n_points=50\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(frontier_df)} portfolio points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot efficient frontier\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Efficient frontier line\n",
    "plt.plot(frontier_df['volatility'], frontier_df['return'], \n",
    "         'b-', linewidth=2, label='Efficient Frontier')\n",
    "\n",
    "# Color points by Sharpe ratio\n",
    "scatter = plt.scatter(\n",
    "    frontier_df['volatility'], \n",
    "    frontier_df['return'],\n",
    "    c=frontier_df['sharpe'],\n",
    "    cmap='viridis',\n",
    "    s=50,\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Mark optimal portfolio\n",
    "plt.scatter(\n",
    "    optimal_sharpe['volatility'],\n",
    "    optimal_sharpe['expected_return'],\n",
    "    c='red',\n",
    "    s=300,\n",
    "    marker='*',\n",
    "    edgecolors='black',\n",
    "    linewidths=2,\n",
    "    label=f\"Optimal (Sharpe={optimal_sharpe['sharpe_ratio']:.2f})\",\n",
    "    zorder=5\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter, label='Sharpe Ratio')\n",
    "plt.xlabel('Portfolio Volatility (Risk)')\n",
    "plt.ylabel('Expected Return')\n",
    "plt.title('Efficient Frontier with Sentiment-Adjusted Returns')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio allocation pie chart\n",
    "weights = optimal_sharpe['weights']\n",
    "non_zero = weights > 0.01\n",
    "filtered_weights = weights[non_zero]\n",
    "filtered_companies = [c for i, c in enumerate(companies) if non_zero[i]]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = plt.cm.Set3(range(len(filtered_companies)))\n",
    "plt.pie(\n",
    "    filtered_weights,\n",
    "    labels=filtered_companies,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    startangle=90,\n",
    "    textprops={'fontsize': 12}\n",
    ")\n",
    "plt.title('Optimal Portfolio Allocation', fontsize=14, fontweight='bold')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-time Alert System\n",
    "\n",
    "Detect sentiment anomalies and generate alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize alert system\n",
    "alert_system = SentimentAlertSystem(\n",
    "    lookback_period=30,\n",
    "    z_threshold=2.0\n",
    ")\n",
    "\n",
    "# Generate all alerts\n",
    "alerts_df = alert_system.get_all_alerts(sentiment_data)\n",
    "\n",
    "print(f\"Generated {len(alerts_df)} alerts\")\n",
    "\n",
    "if len(alerts_df) > 0:\n",
    "    print(\"\\nAlert Summary:\")\n",
    "    print(alerts_df.groupby(['alert_type', 'severity']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display alerts\n",
    "if len(alerts_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETECTED ALERTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for _, alert in alerts_df.iterrows():\n",
    "        severity_icon = \"ðŸ”´\" if alert.get('severity') == 'high' else \"ðŸŸ¡\"\n",
    "        alert_type = alert.get('alert_type', alert.get('reversal_type', 'unknown'))\n",
    "        company = alert.get('company', 'N/A')\n",
    "        \n",
    "        print(f\"\\n{severity_icon} {company} - {alert_type.upper()}\")\n",
    "        print(f\"   Date: {alert.get('date')}\")\n",
    "        \n",
    "        if 'z_score' in alert:\n",
    "            print(f\"   Z-Score: {alert['z_score']:.2f}\")\n",
    "        if 'pct_change' in alert:\n",
    "            print(f\"   Change: {alert['pct_change']:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No alerts detected in the analyzed period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Visualizations\n",
    "\n",
    "Create comprehensive interactive dashboards using Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = EnhancedVisualizer()\n",
    "\n",
    "# Create time series plot with predictions\n",
    "company = 'AAPL'\n",
    "fig = visualizer.plot_sentiment_timeseries(\n",
    "    sentiment_data[company],\n",
    "    company,\n",
    "    predictions=predictions\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment heatmap\n",
    "fig = visualizer.plot_sentiment_heatmap(sentiment_data)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create efficient frontier plot\n",
    "portfolio_results = {\n",
    "    'companies': companies,\n",
    "    'optimal_sharpe': optimal_sharpe,\n",
    "    'efficient_frontier': frontier_df\n",
    "}\n",
    "\n",
    "fig = visualizer.plot_efficient_frontier(frontier_df, optimal_sharpe)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create portfolio allocation chart\n",
    "fig = visualizer.plot_portfolio_allocation(\n",
    "    optimal_sharpe['weights'],\n",
    "    companies\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Pipeline Integration\n",
    "\n",
    "Run the complete end-to-end pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = SentimentPipeline(config={\n",
    "    'lookback_days': 60,\n",
    "    'forecast_days': 7,\n",
    "    'risk_free_rate': 0.02\n",
    "})\n",
    "\n",
    "print(\"Pipeline initialized with:\")\n",
    "print(f\"  Lookback period: {pipeline.lookback_days} days\")\n",
    "print(f\"  Forecast horizon: {pipeline.forecast_days} days\")\n",
    "print(f\"  Risk-free rate: {pipeline.risk_free_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full analysis\n",
    "# Note: This will take several minutes to complete\n",
    "\n",
    "results = pipeline.run_full_analysis(\n",
    "    companies=['AAPL', 'GOOGL', 'MSFT', 'TSLA'],\n",
    "    train_models=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary results\n",
    "print(\"\\nAnalysis Summary:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if 'predictions' in results:\n",
    "    pred_count = len([p for p in results['predictions'].values() if p is not None])\n",
    "    print(f\"âœ“ Predictions generated for {pred_count} companies\")\n",
    "\n",
    "if 'alerts' in results:\n",
    "    alert_count = len(results['alerts'])\n",
    "    print(f\"âœ“ {alert_count} alerts detected\")\n",
    "\n",
    "if 'portfolio' in results:\n",
    "    sharpe = results['portfolio']['optimal_sharpe']['sharpe_ratio']\n",
    "    ret = results['portfolio']['optimal_sharpe']['expected_return']\n",
    "    vol = results['portfolio']['optimal_sharpe']['volatility']\n",
    "    print(f\"âœ“ Portfolio optimized:\")\n",
    "    print(f\"    Sharpe Ratio: {sharpe:.4f}\")\n",
    "    print(f\"    Expected Return: {ret:.4f}\")\n",
    "    print(f\"    Volatility: {vol:.4f}\")\n",
    "\n",
    "if 'visualizations' in results:\n",
    "    viz_count = len(results['visualizations'])\n",
    "    print(f\"âœ“ {viz_count} visualizations created\")\n",
    "\n",
    "print(\"\\nAll outputs saved to /home/claude/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results\n",
    "\n",
    "Save all results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are automatically saved by the pipeline\n",
    "# You can also manually export specific results:\n",
    "\n",
    "# Export predictions\n",
    "if 'predictions' in results:\n",
    "    for company, pred_df in results['predictions'].items():\n",
    "        if pred_df is not None:\n",
    "            pred_df.to_csv(f'/home/claude/predictions_{company}.csv', index=False)\n",
    "            print(f\"Saved predictions for {company}\")\n",
    "\n",
    "# Export portfolio weights\n",
    "if 'portfolio' in results:\n",
    "    weights_df = pd.DataFrame({\n",
    "        'company': results['portfolio']['companies'],\n",
    "        'weight': results['portfolio']['optimal_sharpe']['weights']\n",
    "    })\n",
    "    weights_df.to_csv('/home/claude/optimal_weights.csv', index=False)\n",
    "    print(\"Saved optimal portfolio weights\")\n",
    "\n",
    "# Export alerts\n",
    "if 'alerts' in results and len(results['alerts']) > 0:\n",
    "    results['alerts'].to_csv('/home/claude/alerts.csv', index=False)\n",
    "    print(\"Saved alerts\")\n",
    "\n",
    "print(\"\\nâœ… All results exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Integrate with Real Data**: Replace sample data with actual ensemble output from your FinLlama system\n",
    "2. **Backtesting**: Test strategies on historical data to validate performance\n",
    "3. **Real-time Monitoring**: Set up automated alerts for live trading\n",
    "4. **Custom Models**: Experiment with different architectures (GRU, Transformer, etc.)\n",
    "5. **Multi-asset**: Extend to other asset classes beyond equities\n",
    "\n",
    "For more information, see the README_FUTURE_WORK.md file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
